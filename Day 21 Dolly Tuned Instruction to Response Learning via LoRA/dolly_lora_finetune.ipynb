{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3108c0ba",
   "metadata": {},
   "source": [
    "# Dolly v2-3B LoRA Fine-Tuning\n",
    "This notebook fine-tunes **Databricks' Dolly v2-3B** using the **LaMini-instruction** dataset.\n",
    "LoRA (Low-Rank Adaptation) enables efficient fine-tuning even on smaller GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b870ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q transformers accelerate peft datasets bitsandbytes sentencepiece\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "\n",
    "disable_caching()\n",
    "hf_logging.set_verbosity_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac4142",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"MBZUAI/LaMini-instruction\", split='train')\n",
    "small_dataset = dataset.select(list(range(200)))\n",
    "print(\"Dataset slice:\", small_dataset)\n",
    "print(\"One sample:\", small_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d73c8f",
   "metadata": {},
   "source": [
    "## 3. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = (\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\"\n",
    "    \"Instruction: {instruction}\\nResponse:\"\n",
    ")\n",
    "answer_template = \"{response}\"\n",
    "\n",
    "print(\"Example prompt:\\n\", prompt_template.format(instruction=small_dataset[0]['instruction']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00944080",
   "metadata": {},
   "source": [
    "## 4. Add Prompt/Answer/Text Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _add_text(rec: Dict[str, str]) -> Dict[str, str]:\n",
    "    inst = rec[\"instruction\"]\n",
    "    resp = rec[\"response\"]\n",
    "    if not inst:\n",
    "        raise ValueError(f\"Missing instruction in record: {rec}\")\n",
    "    if not resp:\n",
    "        raise ValueError(f\"Missing response in record: {rec}\")\n",
    "    rec[\"prompt\"] = prompt_template.format(instruction=inst)\n",
    "    rec[\"answer\"] = answer_template.format(response=resp)\n",
    "    rec[\"text\"] = rec[\"prompt\"] + rec[\"answer\"]\n",
    "    return rec\n",
    "\n",
    "small_dataset = small_dataset.map(_add_text)\n",
    "print(\"After adding text field:\", small_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c86f5",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=False\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf49cd9",
   "metadata": {},
   "source": [
    "## 6. Tokenization & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "def _preprocess_batch(batch: Dict[str, List[str]]) -> Dict[str, List]:\n",
    "    tokens = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    tokens[\"labels\"] = copy.deepcopy(tokens[\"input_ids\"])\n",
    "    return tokens\n",
    "\n",
    "encoded_small_dataset = small_dataset.map(\n",
    "    _preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"response\", \"prompt\", \"answer\"]\n",
    ")\n",
    "\n",
    "processed_dataset = encoded_small_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\n",
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "print(\"Dataset split:\", split_dataset)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8,\n",
    "    padding='max_length',\n",
    "    max_length=MAX_LENGTH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5adcade",
   "metadata": {},
   "source": [
    "## 7. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LORA_R = 256\n",
    "LORA_ALPHA = 512\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"query_key_value\"]\n",
    ")\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8fef11",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{MODEL_SAVE_FOLDER_NAME}/logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4efb82",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529b1b1",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a625b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
    "print(f\"âœ… Model saved to {MODEL_SAVE_FOLDER_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad065f01",
   "metadata": {},
   "source": [
    "## 11. Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa45c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess(response: str) -> str:\n",
    "    parts = response.split(\"Response:\")\n",
    "    if len(parts) < 2:\n",
    "        raise ValueError(\"Unexpected response format, expected 'Response:'\")\n",
    "    return \"\".join(parts[1:]).strip()\n",
    "\n",
    "inference_prompt = \"List 5 reasons why someone should learn to cook\"\n",
    "\n",
    "inf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "raw = inf_pipeline(prompt_template.format(instruction=inference_prompt))[0][\"generated_text\"]\n",
    "final_response = postprocess(raw)\n",
    "print(\"Generated response:\\n\", final_response)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}