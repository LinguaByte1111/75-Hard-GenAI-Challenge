{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf92b2c",
   "metadata": {},
   "source": [
    "# ðŸ¦™ Fine-Tuning Meta LLaMA 2 7B with QLoRA on Google Colab\n",
    "This notebook demonstrates **step-by-step fine-tuning of Meta's LLaMA 2 7B model using QLoRA**, an efficient low-memory adaptation method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 1: INSTALL DEPENDENCIES\n",
    "# ========================================\n",
    "!pip install -q bitsandbytes==0.43.1\n",
    "!pip install -q transformers==4.38.2\n",
    "!pip install -q peft==0.10.0\n",
    "!pip install -q accelerate==0.28.0\n",
    "!pip install -q datasets==2.17.1\n",
    "!pip install -q trl==0.8.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a496e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 2: IMPORT LIBRARIES\n",
    "# ========================================\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50035138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 3: LOAD LLAMA 2 MODEL AND TOKENIZER (IN 4-BIT MODE)\n",
    "# ========================================\n",
    "from huggingface_hub import login\n",
    "# login(token=\"YOUR_HUGGINGFACE_TOKEN\")  # Uncomment and paste token here.\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully in 4-bit mode!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 4: CONFIGURE LORA ADAPTER\n",
    "# ========================================\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA adapters added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42347775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 5: LOAD TRAINING DATASET\n",
    "# ========================================\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "dataset = dataset[\"train\"].shuffle(seed=42).select(range(200))\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72514950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 6: DEFINE TRAINING PARAMETERS\n",
    "# ========================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama2-7b-qlora-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e59529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 7: SET UP THE TRAINER\n",
    "# ========================================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=512,\n",
    ")\n",
    "print(\"Trainer initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 8: START TRAINING\n",
    "# ========================================\n",
    "trainer.train()\n",
    "print(\"Training complete! Model and adapters saved to ./llama2-7b-qlora-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80089ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 9: SAVE LORA ADAPTER\n",
    "# ========================================\n",
    "model.save_pretrained(\"./llama2-7b-qlora-adapter\")\n",
    "print(\"LoRA adapter saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a03540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# STEP 10: TEST THE FINE-TUNED MODEL\n",
    "# ========================================\n",
    "prompt = \"Explain quantum computing to a 12-year-old.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
